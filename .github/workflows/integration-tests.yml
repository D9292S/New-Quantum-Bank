name: Integration Tests

on:
  push:
    branches:
      - feature/*
      - build-pipelines
  pull_request:
    branches:
      - main
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'Test suite to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - database
          - api
          - performance

jobs:
  setup-test-matrix:
    runs-on: ubuntu-latest
    outputs:
      test-matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - id: set-matrix
        run: |
          if [[ "${{ github.event.inputs.test_suite }}" == "all" || "${{ github.event.inputs.test_suite }}" == "" ]]; then
            echo "matrix={\"test-suite\":[\"database\",\"api\",\"performance\"]}" >> $GITHUB_OUTPUT
          else
            echo "matrix={\"test-suite\":[\"${{ github.event.inputs.test_suite }}\"]}" >> $GITHUB_OUTPUT
          fi

  integration-tests:
    needs: setup-test-matrix
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix: ${{ fromJson(needs.setup-test-matrix.outputs.test-matrix) }}
    
    env:
      MONGODB_URI: ${{ secrets.MONGODB_URI }}
      
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'
      
      - name: Install UV
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH
      
      - name: Install dependencies
        run: |
          uv pip install --system pytest pytest-asyncio pytest-cov pytest-benchmark
          uv pip install -e . --system
          
          # Install optional performance libraries when testing performance
          if [ "${{ matrix.test-suite }}" == "performance" ]; then
            uv pip install --system uvloop orjson cycler matplotlib
          fi
      
      - name: Run database integration tests
        if: ${{ matrix.test-suite == 'database' }}
        run: |
          echo "Running database integration tests..."
          uv python -m pytest tests/integration/test_mongodb_connection.py -v --junitxml=test-results/database-results.xml
          
          # Run additional database tests
          uv python -m pytest tests/integration/database/ -v --junitxml=test-results/database-additional-results.xml
      
      - name: Run API integration tests
        if: ${{ matrix.test-suite == 'api' }}
        run: |
          echo "Running API integration tests..."
          
          # Start a test instance of the bot with mock configuration
          uv python launcher.py --test-mode --api-only &
          BOT_PID=$!
          
          # Wait for bot to initialize
          sleep 10
          
          # Run API tests
          uv python -m pytest tests/integration/api/ -v --junitxml=test-results/api-results.xml
          
          # Terminate the bot process
          kill $BOT_PID
      
      - name: Run performance tests
        if: ${{ matrix.test-suite == 'performance' }}
        run: |
          echo "Running performance tests..."
          
          # Run performance benchmarks
          uv python -m pytest tests/performance/ --benchmark-json=test-results/benchmark.json -v
          
          # Generate performance report
          uv python tools/run_performance_tests.py --report-only --input test-results/benchmark.json --output test-artifacts/performance-report.md
      
      - name: Upload test results
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.test-suite }}
          path: |
            test-results/
            test-artifacts/
          retention-days: 14
  
  test-summary:
    runs-on: ubuntu-latest
    needs: integration-tests
    if: always()
    steps:
      - uses: actions/checkout@v4
      
      - name: Download all test results
        uses: actions/download-artifact@v4
        with:
          path: all-test-results
      
      - name: Generate test summary
        run: |
          echo "# Integration Test Summary" > test-summary.md
          echo "## Test Results" >> test-summary.md
          
          # Process test results
          find all-test-results -name "*.xml" -type f | while read xml_file; do
            suite_name=$(basename "$xml_file" | sed 's/-results.xml//')
            
            # Extract test counts using grep and sed
            total=$(grep -o 'tests="[0-9]*"' "$xml_file" | sed 's/tests="\([0-9]*\)"/\1/')
            failures=$(grep -o 'failures="[0-9]*"' "$xml_file" | sed 's/failures="\([0-9]*\)"/\1/')
            errors=$(grep -o 'errors="[0-9]*"' "$xml_file" | sed 's/errors="\([0-9]*\)"/\1/')
            skipped=$(grep -o 'skipped="[0-9]*"' "$xml_file" | sed 's/skipped="\([0-9]*\)"/\1/')
            
            # Calculate passed tests
            passed=$((total - failures - errors - skipped))
            
            # Append to summary
            echo "## ${suite_name^} Tests" >> test-summary.md
            echo "- Total: $total" >> test-summary.md
            echo "- Passed: $passed" >> test-summary.md
            echo "- Failed: $failures" >> test-summary.md
            echo "- Errors: $errors" >> test-summary.md
            echo "- Skipped: $skipped" >> test-summary.md
            echo "" >> test-summary.md
          done
          
          # Calculate overall totals
          total_tests=$(find all-test-results -name "*.xml" -type f -exec grep -o 'tests="[0-9]*"' {} \; | sed 's/tests="\([0-9]*\)"/\1/' | awk '{sum+=$1} END {print sum}')
          total_failures=$(find all-test-results -name "*.xml" -type f -exec grep -o 'failures="[0-9]*"' {} \; | sed 's/failures="\([0-9]*\)"/\1/' | awk '{sum+=$1} END {print sum}')
          total_errors=$(find all-test-results -name "*.xml" -type f -exec grep -o 'errors="[0-9]*"' {} \; | sed 's/errors="\([0-9]*\)"/\1/' | awk '{sum+=$1} END {print sum}')
          total_skipped=$(find all-test-results -name "*.xml" -type f -exec grep -o 'skipped="[0-9]*"' {} \; | sed 's/skipped="\([0-9]*\)"/\1/' | awk '{sum+=$1} END {print sum}')
          total_passed=$((total_tests - total_failures - total_errors - total_skipped))
          
          # Calculate success rate
          if [ "$total_tests" -gt 0 ]; then
            success_rate=$(echo "scale=2; ($total_passed * 100) / $total_tests" | bc)
          else
            success_rate="0.00"
          fi
          
          # Append overall summary
          echo "## Overall Summary" >> test-summary.md
          echo "- Total Tests: $total_tests" >> test-summary.md
          echo "- Passed: $total_passed" >> test-summary.md
          echo "- Failed: $total_failures" >> test-summary.md
          echo "- Errors: $total_errors" >> test-summary.md
          echo "- Skipped: $total_skipped" >> test-summary.md
          echo "- Success Rate: ${success_rate}%" >> test-summary.md
      
      - name: Upload test summary
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-summary
          path: test-summary.md
          retention-days: 14
      
      - name: Post summary comment
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            const summary = fs.readFileSync('test-summary.md', 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: summary
            });
            
  create-heroku-deployment-pr:
    needs: [integration-tests, test-summary]
    if: success() && github.event_name == 'push' && github.ref == 'refs/heads/build-pipelines'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          
      - name: Create PR to heroku-deployment
        uses: peter-evans/create-pull-request@v5
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          commit-message: "ci: merge successful integration tests from build-pipelines"
          title: "CI: Merge successful integration tests from build-pipelines"
          body: |
            ## Integration Tests Summary
            
            All integration tests have passed successfully on the `build-pipelines` branch.
            
            This PR proposes merging the validated changes into the `heroku-deployment` branch for deployment.
            
            ### Changes included
            
            - Commit: ${{ github.sha }}
            - Workflow run: ${{ github.run_id }}
          branch: build-pipelines-to-heroku-deployment
          base: heroku-deployment
          labels: |
            automated-pr
            ci-cd-validated
