name: Performance Tests

on:
  push:
    branches:
      - main
      - build-pipelines
  pull_request:
    branches:
      - main
  schedule:
    - cron: '0 0 * * 0'  # Run weekly on Sundays
  workflow_dispatch:

jobs:
  performance-benchmark:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'
          
      - name: Install UV
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH
      
      - name: Install dependencies
        run: |
          uv pip install --system pytest pytest-benchmark pytest-asyncio pytest-cov
          uv pip install --system uvloop orjson cycler matplotlib || true
          uv pip install -e . --system
      
      - name: Run performance benchmarks
        env:
          PERFORMANCE_MODE: high
          OPTIMIZATIONS_AVAILABLE: "true"
          LOG_LEVEL: quiet
          DEVCYCLE_SERVER_SDK_KEY: ${{ secrets.DEVCYCLE_SERVER_SDK_KEY || 'test-key' }}
        run: |
          # Run performance benchmarks
          python -m pytest tests/performance/ --benchmark-json=benchmark.json -v
      
      - name: Generate performance report
        run: |
          python tools/performance_analysis.py --input benchmark.json --output performance-report.md
      
      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: performance-results
          path: |
            benchmark.json
            performance-report.md
          retention-days: 90
          
      - name: Compare with previous benchmark
        if: github.event_name == 'pull_request'
        run: |
          # Download previous benchmark results
          git fetch origin ${{ github.base_ref }}
          git checkout FETCH_HEAD
          
          # If previous benchmark exists, run it for comparison
          if [ -f tests/performance/baseline.json ]; then
            echo "Comparing with baseline performance benchmark..."
            python tools/performance_analysis.py --baseline tests/performance/baseline.json --current benchmark.json --output performance-comparison.md
            
            # Calculate regression percentage
            REGRESSION=$(python -c "
            import json
            with open('performance-comparison.md', 'r') as f:
                content = f.read()
            print('1' if 'REGRESSION' in content else '0')
            ")
            
            if [ "$REGRESSION" == "1" ]; then
              echo "::warning::Performance regression detected! See the uploaded artifacts for details."
            else
              echo "âœ… No performance regression detected."
            fi
          else
            echo "No baseline benchmark found. This will be the first benchmark."
          fi
      
      - name: Comment on PR with performance results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          script: |
            const fs = require('fs');
            
            let body = '## Performance Benchmark Results\n\n';
            
            if (fs.existsSync('performance-comparison.md')) {
              body += fs.readFileSync('performance-comparison.md', 'utf8');
            } else {
              body += 'First benchmark run - no comparison available.\n';
              body += fs.readFileSync('performance-report.md', 'utf8');
            }
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: body
            }); 